{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78fe471a",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0fe10d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/scratch/project_2001083/nasib/Cache/'\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "#from apex import amp\n",
    "from model import LightXML\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import AdamW\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from dataset import MDataset, createDataCSV\n",
    "from log import Logger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b63651",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44669ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_group(dataset, group_tree=0):\n",
    "    if dataset == 'wiki500k':\n",
    "        return np.load(f'/scratch/project_2001083/nasib/XMC/LightXML/data/Wiki-500K/label_group{group_tree}.npy', allow_pickle=True)\n",
    "    if dataset == 'amazon670k':\n",
    "        return np.load(f'/scratch/project_2001083/nasib/XMC/LightXML/data/Amazon-670K/label_group{group_tree}.npy', allow_pickle=True)\n",
    "    if dataset == 'amazontitles300k':\n",
    "        return np.load(f'/scratch/project_2001083/nasib/XMC/LightXML/data/AmazonTitles-300K/label_group{group_tree}.npy', allow_pickle=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9018b1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "grp_y = load_group('amazontitles300k')\n",
    "len(grp_y),len(grp_y[0]),len(grp_y[-1]),sum([len(y) for y in grp_y])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3981f1c5",
   "metadata": {},
   "source": [
    "# Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b07f563",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, df, label_map):\n",
    "    tokenizer = model.get_tokenizer()\n",
    "\n",
    "    if args.dataset in ['wiki500k', 'amazon670k','amazontitles300k']:\n",
    "        group_y = load_group(args.dataset, args.group_y_group)\n",
    "        train_d = MDataset(df, 'train', tokenizer, label_map, args.max_len, group_y=group_y,\n",
    "                           candidates_num=args.group_y_candidate_num)\n",
    "        test_d = MDataset(df, 'test', tokenizer, label_map, args.max_len, group_y=group_y,\n",
    "                           candidates_num=args.group_y_candidate_num)\n",
    "        \n",
    "        #print(len(train_d[0]),train_d[0])\n",
    "        print(len(test_d[0]),test_d[0])\n",
    "\n",
    "        #train_d.tokenizer = model.get_fast_tokenizer()\n",
    "        #test_d.tokenizer = model.get_fast_tokenizer()\n",
    "\n",
    "        trainloader = DataLoader(train_d, batch_size=args.batch, num_workers=5,\n",
    "                                 shuffle=True)\n",
    "        testloader = DataLoader(test_d, batch_size=args.batch, num_workers=5,\n",
    "                                shuffle=False)\n",
    "        if args.valid:\n",
    "            valid_d = MDataset(df, 'valid', tokenizer, label_map, args.max_len, group_y=group_y,\n",
    "                               candidates_num=args.group_y_candidate_num)\n",
    "            validloader = DataLoader(valid_d, batch_size=args.batch, num_workers=0, \n",
    "                                     shuffle=False)\n",
    "    else:\n",
    "        train_d = MDataset(df, 'train', tokenizer, label_map, args.max_len)\n",
    "        test_d = MDataset(df, 'test', tokenizer, label_map, args.max_len)\n",
    "        trainloader = DataLoader(train_d, batch_size=args.batch, num_workers=2,\n",
    "                                 shuffle=True)\n",
    "        testloader = DataLoader(test_d, batch_size=args.batch, num_workers=1,\n",
    "                                shuffle=False)\n",
    "\n",
    "    model.cuda()\n",
    "    no_decay = ['bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.lr)#, eps=1e-8)\n",
    "        \n",
    "    #model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\")\n",
    "\n",
    "    max_only_p5 = 0\n",
    "    for epoch in range(0, 1):\n",
    "        train_loss = model.one_epoch(epoch, trainloader, optimizer, mode='train',\n",
    "                                     eval_loader=validloader if args.valid else testloader,\n",
    "                                     eval_step=args.eval_step, log=LOG)\n",
    "\n",
    "        if args.valid:\n",
    "            ev_result = model.one_epoch(epoch, validloader, optimizer, mode='eval')\n",
    "        else:\n",
    "            ev_result = model.one_epoch(epoch, testloader, optimizer, mode='eval')\n",
    "\n",
    "        g_p1, g_p3, g_p5, p1, p3, p5 = ev_result\n",
    "\n",
    "        log_str = f'{epoch:>2}: {p1:.4f}, {p3:.4f}, {p5:.4f}, train_loss:{train_loss}'\n",
    "        if args.dataset in ['wiki500k', 'amazon670k','amazontitles300k']:\n",
    "            log_str += f' {g_p1:.4f}, {g_p3:.4f}, {g_p5:.4f}'\n",
    "        if args.valid:\n",
    "            log_str += ' valid'\n",
    "        LOG.log(log_str)\n",
    "\n",
    "        if max_only_p5 < p5:\n",
    "            max_only_p5 = p5\n",
    "            model.save_model(f'models/model-{get_exp_name()}.bin')\n",
    "\n",
    "        if epoch >= args.epoch + 5 and max_only_p5 != p5:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3a3661",
   "metadata": {},
   "source": [
    "# Runing Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a0e1f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.batch = 200\n",
    "        self.update_count = 1\n",
    "        self.lr = .0001\n",
    "        self.seed = 6088\n",
    "        self.epoch = 20\n",
    "        self.dataset = 'amazontitles300k' #amazontitles300k\n",
    "        self.bert = 'bert-base'\n",
    "        self.max_len = 128\n",
    "        self.valid = False #make validation split\n",
    "        self.swa = True\n",
    "        self.swa_warmup = 4\n",
    "        self.swa_step = 50\n",
    "        \n",
    "        self.group_y_group = 0\n",
    "        self.group_y_candidate_num = 2000\n",
    "        self.group_y_candidate_topk = 75\n",
    "        self.eval_step = 3000\n",
    "        self.hidden_dim = 400\n",
    "        self.eval_model =False #\n",
    "        \n",
    "args = Config()\n",
    "\n",
    "def get_exp_name():\n",
    "    name = [args.dataset, '' if args.bert == 'bert-base' else args.bert]\n",
    "    if args.dataset in ['wiki500k', 'amazon670k','amazontitles300k']:\n",
    "        name.append('t'+str(args.group_y_group))\n",
    "\n",
    "    return '_'.join([i for i in name if i != ''])\n",
    "\n",
    "\n",
    "def init_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b1b3b6",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15361ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amazontitles300k_t0\n",
      "Running model for the Configuration: <__main__.Config object at 0x7f452ed2fc70>\n",
      "load amazontitles300k dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "586781it [00:00, 1675429.94it/s]\n",
      "260536it [00:00, 1710583.91it/s]\n",
      "586781it [00:01, 567122.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "260536it [00:00, 601700.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303296\n",
      "label map 303296\n",
      "load amazontitles300k dataset with 586781 train 260536 test with 303296 labels done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/run/nvme/job_17795992/tmp/ipykernel_2660013/3685890099.py:35: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  group_y = np.array(_group_y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swa True 4 50 {}\n",
      "update_count 1\n",
      "load bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden dim: 400\n",
      "label group numbers: 4096\n",
      "load bert-base-uncased tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/project_2001083/nasib/XMC/LightXML/src/dataset.py:88: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  self.group_y = np.array(self.group_y)\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/usr/local/lib64/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 (tensor([  101,  9004,  2865, 15550,  2098,  6887, 12356,  2015,  1996,  3733,\n",
      "         2126,  2000,  6570,  2115,  4743,  2000,  3713,  3872,  1015,  1024,\n",
      "         5986,  2616,  1998, 15672,   102,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), tensor([0., 0., 1.,  ..., 0., 0., 0.]), tensor([0., 0., 0.,  ..., 0., 0., 0.]), array([113225, 242743, 240521, ..., 205113, 201113,  62191]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "train-0:  46%|████▌     | 1342/2934 [32:19<38:18,  1.44s/it, loss=0.0161] "
     ]
    }
   ],
   "source": [
    "init_seed(args.seed)\n",
    "\n",
    "print(get_exp_name())\n",
    "\n",
    "#log_name = 'log_'+str(args.run_name)\n",
    "#print('Log file name: ',log_name)\n",
    "\n",
    "LOG = Logger('test')\n",
    "\n",
    "args.valid = False\n",
    "\n",
    "print('Running model for the Configuration:',args)\n",
    "\n",
    "print(f'load {args.dataset} dataset...')\n",
    "df, label_map = createDataCSV(args.dataset)\n",
    "#args.valid = False\n",
    "if args.valid:\n",
    "    train_df, valid_df = train_test_split(df[df['dataType'] == 'train'],\n",
    "                                            test_size=4000,\n",
    "                                            random_state=1240)\n",
    "    df.iloc[valid_df.index.values, 2] = 'valid'\n",
    "    print('valid size', len(df[df['dataType'] == 'valid']))\n",
    "\n",
    "print(f'load {args.dataset} dataset with '\n",
    "        f'{len(df[df.dataType ==\"train\"])} train {len(df[df.dataType ==\"test\"])} test with {len(label_map)} labels done')\n",
    "\n",
    "if args.dataset in ['wiki500k', 'amazon670k','amazontitles300k']:\n",
    "    group_y = load_group(args.dataset, args.group_y_group)\n",
    "    _group_y = []\n",
    "    for idx, labels in enumerate(group_y):\n",
    "        _group_y.append([])\n",
    "        for label in labels:\n",
    "            _group_y[-1].append(label_map[label])\n",
    "        _group_y[-1] = np.array(_group_y[-1])\n",
    "    group_y = np.array(_group_y)\n",
    "\n",
    "    model = LightXML(n_labels=len(label_map), group_y=group_y, bert=args.bert,\n",
    "                        update_count=args.update_count,\n",
    "                        use_swa=args.swa, swa_warmup_epoch=args.swa_warmup, swa_update_step=args.swa_step,\n",
    "                        candidates_topk=args.group_y_candidate_topk,\n",
    "                        hidden_dim=args.hidden_dim)\n",
    "else:\n",
    "    model = LightXML(n_labels=len(label_map), bert=args.bert,\n",
    "                        update_count=args.update_count,\n",
    "                        use_swa=args.swa, swa_warmup_epoch=args.swa_warmup, swa_update_step=args.swa_step)\n",
    "\n",
    "if args.eval_model and args.dataset in ['wiki500k', 'amazon670k','amazontitles300k']:\n",
    "    print(f'load models/model-{get_exp_name()}.bin')\n",
    "    testloader = DataLoader(MDataset(df, 'test', model.get_fast_tokenizer(), label_map, args.max_len, \n",
    "                                        candidates_num=args.group_y_candidate_num),\n",
    "                            batch_size=256, num_workers=0, \n",
    "                            shuffle=False)\n",
    "\n",
    "    group_y = load_group(args.dataset, args.group_y_group)\n",
    "    validloader = DataLoader(MDataset(df, 'valid', model.get_fast_tokenizer(), label_map, args.max_len, group_y=group_y,\n",
    "                                        candidates_num=args.group_y_candidate_num),\n",
    "                                batch_size=256, num_workers=0, \n",
    "                        shuffle=False)\n",
    "    model.load_state_dict(torch.load(f'models/model-{get_exp_name()}.bin'))\n",
    "    model = model.cuda()\n",
    "\n",
    "    print(len(df[df.dataType == 'test']))\n",
    "    model.one_epoch(0, validloader, None, mode='eval')\n",
    "\n",
    "    pred_scores, pred_labels = model.one_epoch(0, testloader, None, mode='test')\n",
    "    np.save(f'results/{get_exp_name()}-labels.npy', np.array(pred_labels))\n",
    "    np.save(f'results/{get_exp_name()}-scores.npy', np.array(pred_scores))\n",
    "    sys.exit(0)\n",
    "\n",
    "train(model, df, label_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f672b8b3",
   "metadata": {},
   "source": [
    "# Making the Cluster labels for each Cluster and Creation of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc917b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.dataset in ['wiki500k', 'amazon670k','amazontitles300k']:\n",
    "    group_y = load_group(args.dataset, args.group_y_group)\n",
    "    _group_y = []\n",
    "    for idx, labels in enumerate(group_y):\n",
    "        _group_y.append([])\n",
    "        for label in labels:\n",
    "            _group_y[-1].append(label_map[label])\n",
    "        _group_y[-1] = np.array(_group_y[-1])\n",
    "    group_y = np.array(_group_y)\n",
    "    \n",
    "    \n",
    "    model = LightXML(n_labels=len(label_map), group_y=group_y, bert=args.bert,\n",
    "                  update_count=args.update_count,\n",
    "                  use_swa=args.swa, swa_warmup_epoch=args.swa_warmup, swa_update_step=args.swa_step,\n",
    "                  candidates_topk=args.group_y_candidate_topk,\n",
    "                  hidden_dim=args.hidden_dim)\n",
    "    \n",
    "train(model, df, label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10da02d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
